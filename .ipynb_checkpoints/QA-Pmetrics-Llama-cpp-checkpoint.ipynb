{
 "cells": [
  {
   "attachments": {
    "3dce65bf-ad2d-4d56-8dca-80fcd28c0349.webp": {
     "image/webp": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3bca21f7-800f-46aa-aab9-9ee520a90469",
   "metadata": {},
   "source": [
    "![1_mdzyMcDhL0YLtgtfw7eaaA.webp](attachment:3dce65bf-ad2d-4d56-8dca-80fcd28c0349.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d688950-984e-472e-8e9f-f39f9435d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b407ef76-e854-415e-a5f2-9744a181e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "import langchain\n",
    "import llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb38a7a-0832-49fd-bd80-d40f3226a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/entityjy/llama2/llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.gguf.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama-2-7b-chat.ggmlv3.q4_K_M.bin\n",
      "llama_model_loader: - kv   2:                        general.description str              = converted from legacy GGJTv3 MOSTLY_Q...\n",
      "llama_model_loader: - kv   3:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv   4:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name   = llama-2-7b-chat.ggmlv3.q4_K_M.bin\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5461.34 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 11.88 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3891.95 MiB, ( 3892.45 /  5461.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.02 MiB, ( 4148.47 /  5461.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =     8.83 MiB, ( 4157.30 /  5461.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# /Users/entityjy/llama2/llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.gguf.bin\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "# model_config = transformers.AutoConfig.from_pretrained(\n",
    "#     \"/Users/entityjy/llama2/llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.gguf.bin\",\n",
    "# )\n",
    "\n",
    "n_gpu_layers = 1\n",
    "n_batch = 64\n",
    "\n",
    "model = langchain.LlamaCpp(\n",
    "    model_path=\"/Users/entityjy/llama2/llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.gguf.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=512,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    "    # repeat_penalty=1.1,\n",
    "    # temperature=0.3,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e205c58-2aad-4d50-847f-214ee91ffd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b01edb4-2f9c-43ff-ad4d-45511e8f8995",
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1002)')))\"), '(Request ID: 3aec455d-9407-4ead-8169-1a8349f83c37)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1092\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    635\u001b[0m         (\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[0;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connection.py:783\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    781\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 783\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/util/ssl_.py:469\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/util/ssl_.py:513\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1002)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1002)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1002)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m stop_token_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m stop_list:\n\u001b[0;32m----> 6\u001b[0m     stop_token_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/language_model.py:236\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ordered ids of the tokens in a text.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m            in the text.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/language_model.py:39\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1748\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 1748\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProxyError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/langchain/lib/python3.11/site-packages/requests/adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1002)')))\"), '(Request ID: 3aec455d-9407-4ead-8169-1a8349f83c37)')"
     ]
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n', '\\n\\n\\n']\n",
    "\n",
    "stop_token_ids = []\n",
    "\n",
    "for n in stop_list:\n",
    "    stop_token_ids.append(model.get_token_ids(n))\n",
    "\n",
    "\n",
    "# stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5200e71-895a-453b-a59e-63bd21291d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d5cd8-d242-4f71-a22c-081a976f07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8c080-2389-4964-a57d-5ac189c00de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.3,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5ef68-6afb-434f-838c-0337fe15c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error seems to come from quantization not being initiiated on the parts in the cpu or parts in mps\n",
    "res = generate_text(\"Question: what is a PM_data object in Pmetrics? Answer:\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a920a1a-26aa-4017-8a0c-0c8cef945321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"what is a PM_data object in Pmetrics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1084c-63dd-43c6-a708-81dc9d7d81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://lapkb.github.io/Pmetrics/reference/plot.PMvalid.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMFortranConfig.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_valid.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMcheck.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/SIMrun.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMmatrix.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ITparse.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/simEx.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/update_gfortran.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMcov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/SIMparse.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_pta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_fit.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ger_bmi.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.summary.PMmatrix.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeFinal.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/NM2PM.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/running.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_cov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/MM_opt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMmb2csv.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_cov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/modelLibrary.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMupdate.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/pta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ss.PK.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_batch.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMop.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/Dopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/Pmetrics-package.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/export_plotly.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makePTAtarget.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMsim.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMlogin.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMcycle.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makePop.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/mic1.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PM_cov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMdopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/dataEx.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMwrk2csv.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/intro.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMbuild.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/qgrowth.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PM_data.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMop.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMlogout.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/objects.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/index.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_cycle.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_report.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/pipe.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/pmetrics.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/add_shapes.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/models.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_valid.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeOP.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.PMdopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/build_model.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/authors.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.PMnpc.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/badData.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.summary.PMfinal.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMpta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_pta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMload.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/setPMoptions.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMsave.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/cdc_bmi.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/mtsknn.eq.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.MMopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/index.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_pop.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeAUC.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_data.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMfinal.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMstep.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_model.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/data.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/add_smooth.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/NPparse.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_final.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ITex.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_simlist.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/editPMoptions.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMtree.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMmatrixRelTime.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeCov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/plotly.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.PMerr.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/zBMI.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/make_valid.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMgetCRCL.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makePTA.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/overview.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_simlist.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMpatch.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_op.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PM_op.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ERRreport.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_sim.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_model.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/NPrun.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/getPMoptions.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/make_NCA.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.summary.PMop.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMreadMatrix.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeErrorPoly.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMfinal.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMcov.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_sim.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeNCA.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeCycle.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_cycle.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PM_data.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makePost.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/NPex.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ITrun.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ERRrun.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMwriteMatrix.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/growth.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/movePMoptions.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMtest.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMpta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PM_final.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_final.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_result.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_post.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMnews.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/modEx.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMcompare.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/makeValid.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/plot.PMdopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PMmatrix.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/sub_plot.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMreport.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/summary.PM_pta.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_load.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_parse.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_tutorial.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/index.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/ab_line.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/workflow.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/articles/simulation.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMregister.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/print.MMopt.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PM_op.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMmanual.html\",\n",
    "\"https://lapkb.github.io/Pmetrics/reference/PMcode.html\"] \n",
    "#Data ingesting by Scraping\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5032ca-dd9a-4c6c-bb2c-8b40c3dd34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff2ffd-f5e8-4c23-b63f-815008ff673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"mps\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e83b4-1ab0-4c28-ada3-a28c3df8a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9c1cc-36fc-4a8d-8601-bb0478259928",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"explain in detail what is a PM_data object in Pmetrics?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e531760-9f7e-450d-a9ce-9375dcb60762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af567651-c7bd-43c5-aec4-bc5f4c85a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_history = [(query, result[\"answer\"])]\n",
    "chat_history = []\n",
    "\n",
    "query = \"what is Pmetrics?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e8e11-2b0f-4cbe-b4a2-7b4a0f218c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ad08d-e5d2-4871-9ec9-66d5b98f98e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"how can I create a data object in Pmetrics?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e806c-081e-4268-bf7a-d012c6f4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87a8dc-de80-4705-b1ec-2e81c154b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"what is R6 Pmetrics?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c0595-7931-4705-8546-f4272cf7a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329f43f-3ea9-4e01-ab1f-e6239b7d7810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"who wrote Pmetrics\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f1605-a647-4ed7-9696-cc061bcf7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef54d1-85e2-4376-a314-93975b311a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fe77e-0522-4d01-83e7-91afe4066f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"\"\"Please provide a detailed explanation of the differences between parametric and non-parametric statistical approaches in pharmacometrics,\n",
    "           including their strengths and limitations, and examples of when each approach might be appropriate\"\"\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60babb7d-12d3-4194-b273-b496f9315695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
